{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D mask proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scripts.utils import load_ply\n",
    "from pytorch3d.structures import Meshes,Pointclouds\n",
    "from pytorch3d.renderer import Textures\n",
    "from pytorch3d.io import load_obj\n",
    "from point_sam.build_model import build_point_sam\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import pytorch3d.ops as ops\n",
    "from pytorch3d.ops import sample_farthest_points\n",
    "from utils.nms import apply_pointwise_nms,visualize_point_clouds_with_masks\n",
    "from mask_proposal import  mask_proposal,mask_proposal_v2,batch_mask_proposal\n",
    "from utils.render import render_all_angles_pc,render_single_view,project_3d_to_2d\n",
    "import glob\n",
    "from point_sam.build_model import build_point_sam\n",
    "import numpy as np\n",
    "import torch\n",
    "from scripts.utils import load_ply\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import Textures\n",
    "from pytorch3d.io import load_obj\n",
    "# Use glob to access all files in the directory\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import random\n",
    "import os\n",
    "from utils.inference_florence import run_florence2\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import supervision as sv\n",
    "import open3d as o3d\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_prediction_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [{'file': parts[0], 'prediction': int(parts[1]), 'confidence': float(parts[2])}\n",
    "                for line in file if len(parts := line.strip().split()) == 3]\n",
    "\n",
    "def normalize_point_cloud(xyz):\n",
    "    centroid = np.mean(xyz, axis=0)\n",
    "    xyz_centered = xyz - centroid\n",
    "    furthest_distance = np.max(np.sqrt(np.sum(xyz_centered**2, axis=1)))\n",
    "    return xyz_centered / furthest_distance\n",
    "\n",
    "def process_scene(scene_id, scene_path, mask_info_path, model, output_dir,mask_infos):\n",
    "    pcd = o3d.io.read_point_cloud(scene_path)\n",
    "    xyz = np.asarray(pcd.points)\n",
    "    rgb = np.asarray(pcd.colors) * 255\n",
    "\n",
    "#     rotation_matrix = o3d.geometry.get_rotation_matrix_from_xyz((-np.pi/2, 0, 0))\n",
    "\n",
    "# # Rotate point cloud\n",
    "\n",
    "# # Rotate mesh\n",
    "#     pcd = pcd.rotate(rotation_matrix, center=(0, 0, 0)) \n",
    "    \n",
    "    for idx, mask_info in enumerate(mask_infos):\n",
    "        mask = np.loadtxt(os.path.join(os.path.dirname(mask_info_path), mask_info['file'])).astype(bool)\n",
    "        obj_xyz = normalize_point_cloud(xyz[mask])\n",
    "        obj_rgb = rgb[mask]\n",
    "        \n",
    "        obj_xyz_tensor = torch.tensor(obj_xyz).to(device).float()\n",
    "        obj_rgb_tensor = torch.tensor(obj_rgb).to(device).float()\n",
    "\n",
    "         \n",
    "        obj_pcd = Pointclouds(points=[obj_xyz_tensor], features=[obj_rgb_tensor])\n",
    "        obj_xyz_tensor = obj_xyz_tensor.unsqueeze(0)\n",
    "        obj_rgb_tensor = obj_rgb_tensor.unsqueeze(0)\n",
    "        top_k_masks, _, _ = mask_proposal(obj_xyz_tensor, obj_rgb_tensor, NUM_PROMPTS, model)\n",
    "        #instance_pcd\n",
    "        img_dir, pc_depth, screen_coords, num_views, cameras = render_all_angles_pc(obj_pcd, os.path.join(output_dir, str(idx)), device)\n",
    "        # save top_k_masks,pc_depth,screen_coords as pt\n",
    "        # save obj_xyz as np\n",
    "        # make a new directoy under the os.path.join(output_dir, str(idx)) called ins_info\n",
    "        instance_info_dir = os.path.join(output_dir, str(idx), 'ins_info')\n",
    "        os.makedirs(instance_info_dir, exist_ok=True)\n",
    "\n",
    "        # Save top_k_masks, pc_depth, and screen_coords as pt files\n",
    "        torch.save(top_k_masks, os.path.join(instance_info_dir, 'top_k_masks.pt'))\n",
    "        torch.save(pc_depth, os.path.join(instance_info_dir, 'pc_depth.pt'))\n",
    "        torch.save(screen_coords, os.path.join(instance_info_dir, 'screen_coords.pt'))\n",
    "\n",
    "        # Save obj_xyz as numpy array\n",
    "        np.save(os.path.join(instance_info_dir, 'obj_xyz.npy'), obj_xyz)\n",
    "\n",
    "        print(f\"Saved instance information and point cloud data to {instance_info_dir}\")\n",
    "    return top_k_masks, img_dir, pc_depth, screen_coords, num_views, cameras,obj_xyz\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    NUM_PROMPTS = 1024\n",
    "    NUM_MASKS_PER_PROMPT = 3\n",
    "    NMS_THRESHOLD = 0.3\n",
    "    TOP_K_PROPOSALS = 250\n",
    "\n",
    "    \n",
    "    dataset_dir = '/home/wan/Datasets/Test_scene/part_valid'\n",
    "    project_path = '/home/wan/Workplace-why/PartScene'\n",
    "    final_masks_save_dir = os.path.join(project_path, 'part_scene_results')\n",
    "    by_product_save_dir = 'part_scene_saved'\n",
    "    ckpt_path = os.path.join(project_path, \"checkpoints/model.safetensors\")\n",
    "\n",
    "    model = build_point_sam(ckpt_path, 512, 64).to(device)\n",
    "    print('Model built successfully')\n",
    "\n",
    "    for scene_id in os.listdir(dataset_dir):\n",
    "        print(scene_id)\n",
    "        scene_path = os.path.join(dataset_dir, scene_id, f'points_{scene_id}.ply')\n",
    "        mask_info_path = os.path.join(final_masks_save_dir, scene_id, f'{scene_id}_summary.txt')\n",
    "        output_dir = os.path.join(project_path, by_product_save_dir, scene_id)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        mask_infos = load_prediction_data(mask_info_path)\n",
    "\n",
    "        # i\n",
    "        top_k_masks,img_dir, pc_depth, screen_coords, num_views, cameras,obj_xyz  = process_scene(scene_id, scene_path, mask_info_path, model, output_dir,mask_infos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scripts.utils import load_ply\n",
    "from pytorch3d.structures import Meshes,Pointclouds\n",
    "from pytorch3d.renderer import Textures\n",
    "from pytorch3d.io import load_obj\n",
    "from point_sam.build_model import build_point_sam\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "# Use glob to access all files in the directory\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from third_party.torkit3d.config.config import * \n",
    "import random\n",
    "import os\n",
    "from utils.inference_florence import run_florence2\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import supervision as sv\n",
    "from utils.utils_3d import * \n",
    "from third_party.Ground_SAM.sam2.build_sam import build_sam2\n",
    "from third_party.Ground_SAM.sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from third_party.Ground_SAM.mask_proposal_2d import segment2d\n",
    "import open3d as o3d\n",
    "import json \n",
    "\n",
    "\n",
    "def load_instance_info(instance_info_dir):\n",
    "    \"\"\"\n",
    "    Load instance information and point cloud data from the given directory.\n",
    "    \n",
    "    Args:\n",
    "    instance_info_dir (str): Path to the directory containing the saved files.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the loaded data.\n",
    "    \"\"\"\n",
    "    # Load PyTorch tensors\n",
    "    top_k_masks = torch.load(os.path.join(instance_info_dir, 'top_k_masks.pt'))\n",
    "    pc_depth = torch.load(os.path.join(instance_info_dir, 'pc_depth.pt'))\n",
    "    screen_coords = torch.load(os.path.join(instance_info_dir, 'screen_coords.pt'))\n",
    "    \n",
    "    # Load numpy array\n",
    "    obj_xyz = np.load(os.path.join(instance_info_dir, 'obj_xyz.npy'))\n",
    "    \n",
    "    # Create a dictionary to hold all the loaded data\n",
    "    \n",
    "    \n",
    "    \n",
    "    return top_k_masks,pc_depth,screen_coords,obj_xyz\n",
    "\n",
    "\n",
    "\n",
    "import os \n",
    "\n",
    "\n",
    "       \n",
    "FLORENCE2_MODEL_ID = \"microsoft/Florence-2-large\"\n",
    "SAM2_CHECKPOINT = \"/home/wan/Workplace-why/PartScene/third_party/Ground_SAM/checkpoints/sam2_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "reversed_dict = {value: key for key, value in cls_dict.items()}\n",
    "\n",
    "# build florence-2\n",
    "florence2_model = AutoModelForCausalLM.from_pretrained(FLORENCE2_MODEL_ID, trust_remote_code=True, torch_dtype='auto').eval().to(device)\n",
    "florence2_processor = AutoProcessor.from_pretrained(FLORENCE2_MODEL_ID, trust_remote_code=True)\n",
    "# build sam 2\n",
    "sam2_model = build_sam2(SAM2_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_dir = '/home/wan/Datasets/Test_scene/part_valid'\n",
    "project_path = '/home/wan/Workplace-why/PartScene'\n",
    "final_masks_save_dir = os.path.join(project_path, 'part_scene_results')\n",
    "by_product_save_dir = 'part_scene_saved'\n",
    "ckpt_path = os.path.join(project_path, \"checkpoints/model.safetensors\")\n",
    "\n",
    "\n",
    "for scene_id in tqdm(os.listdir(dataset_dir)):\n",
    "        print(scene_id)\n",
    "        scene_path = os.path.join(dataset_dir, scene_id, f'points_{scene_id}.ply')\n",
    "        mask_result_path = os.path.join(final_masks_save_dir, scene_id)\n",
    "        output_scene_dir = os.path.join(project_path, by_product_save_dir, scene_id)\n",
    "        mask_infos = load_prediction_data( f'{mask_result_path}/{scene_id}_summary.txt')\n",
    "        for idx,mask in enumerate(mask_infos):\n",
    "                ins_num = mask['prediction']\n",
    "                instance_dir = os.path.join(output_scene_dir,str(idx))\n",
    "                top_k_masks,pc_depth,screen_coords,obj_xyz = load_instance_info(f'{instance_dir}/ins_info')\n",
    "                ins =reversed_dict[ins_num]\n",
    "                prompt = cls_part_dict[ins]\n",
    "                file_paths = glob.glob(os.path.join(f'{instance_dir}/rendered_img', '*'))\n",
    "                points_3d =[]\n",
    "                visible_pts_list = []\n",
    "                # Print all the files found\n",
    "                num_views = pc_depth.shape[0]\n",
    "                task_prompt = \"<OPEN_VOCABULARY_DETECTION>\"\n",
    "                text_input = prompt\n",
    "                # torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__(\n",
    "                torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "                result_dict = segment2d(num_views = num_views ,save_dir=instance_dir,text_input=text_input,task_prompt=task_prompt,florence2_model=florence2_model,florence2_processor=florence2_processor,sam2_predictor= sam2_predictor)\n",
    "                torch.save(result_dict, os.path.join(f'{instance_dir}/ins_info', 'sem_seg.pt'))\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_3d import * \n",
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d \n",
    "from matplotlib import pyplot as plt \n",
    "import os \n",
    "import json\n",
    "import glob\n",
    "from utils.process import *\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "from third_party.torkit3d.config.config import * \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mask_results(scene_id, part_mask_after_process, scene_pcd, ins_mask, ins, output_dir, part_label_v2):\n",
    "    try:\n",
    "        scene_dir = os.path.join(output_dir, f'{scene_id}')\n",
    "        pred_part_mask_dir = os.path.join(scene_dir, 'pred_part_mask')\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to create directory: {pred_part_mask_dir}\")\n",
    "        print(\"Please check that you have write permissions for the output directory.\")\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Output directory path: {output_dir}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while creating directories: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    summary_data = []\n",
    "    base_cls = ins.split(' ')[-1].lower()\n",
    "\n",
    "    # Get the current highest index in the pred_part_mask directory\n",
    "    existing_files = glob.glob(os.path.join(pred_part_mask_dir, '*.txt'))\n",
    "    numeric_files = [f for f in existing_files if re.match(r'^\\d+\\.txt$', os.path.basename(f))]\n",
    "    if numeric_files:\n",
    "        highest_idx = max([int(os.path.splitext(os.path.basename(f))[0]) for f in numeric_files])\n",
    "        start_idx = highest_idx + 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    for idx, (label_key, data) in enumerate(part_mask_after_process.items(), start=start_idx):\n",
    "        part_mask = data['mask']\n",
    "        part_score = data['score']\n",
    "        \n",
    "        # Map parts to scene\n",
    "        scene_part_mask = map_parts_to_scene(scene_pcd, part_mask, ins_mask)\n",
    "        \n",
    "        # Save individual mask file\n",
    "        mask_filename = f'{idx:03d}.txt'\n",
    "        mask_filepath = os.path.join(pred_part_mask_dir, mask_filename)\n",
    "        \n",
    "        # Convert scene_part_mask to integer numpy array\n",
    "        if isinstance(scene_part_mask, np.ndarray):\n",
    "            mask_to_save = scene_part_mask.astype(int)\n",
    "        else:\n",
    "            mask_to_save = np.array(scene_part_mask, dtype=int)\n",
    "        \n",
    "        try:\n",
    "            # Save the mask\n",
    "            np.savetxt(mask_filepath, mask_to_save, fmt='%d')\n",
    "        except PermissionError:\n",
    "            print(f\"Error: Permission denied when trying to save file: {mask_filepath}\")\n",
    "            print(\"Please check that you have write permissions for the output directory.\")\n",
    "            sys.exit(1)\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while saving mask file: {str(e)}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Get part label and number\n",
    "        part_label = f'{base_cls}_{label_key}'\n",
    "        # print(part_label)\n",
    "        \n",
    "        part_label_num = part_label_v2[part_label]  # This will return None if label not found\n",
    "        # Append to summary data\n",
    "        summary_data.append(f\"pred_part_mask/{mask_filename} {part_label_num} {part_score:.4f}\")\n",
    "    \n",
    "    # Save part summary file at the same level as regular summary\n",
    "    part_summary_filepath = os.path.join(scene_dir, f'{scene_id}_part_summary.txt')\n",
    "    \n",
    "    try:\n",
    "        # Append to part summary file\n",
    "        with open(part_summary_filepath, 'a') as f:\n",
    "            f.write('\\n'.join(summary_data) + '\\n')\n",
    "        print(f\"Part summary appended in {part_summary_filepath}\")\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when trying to save file: {part_summary_filepath}\")\n",
    "        print(\"Please check that you have write permissions for the output directory.\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while saving part summary file: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]/home/wan/Workplace-why/Part-SAM/utils/utils_3d.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  top_k_masks = torch.load(os.path.join(instance_info_dir, 'top_k_masks.pt'))\n",
      "/home/wan/Workplace-why/Part-SAM/utils/utils_3d.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pc_depth = torch.load(os.path.join(instance_info_dir, 'pc_depth.pt'))\n",
      "/home/wan/Workplace-why/Part-SAM/utils/utils_3d.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  screen_coords = torch.load(os.path.join(instance_info_dir, 'screen_coords.pt'))\n",
      "/tmp/ipykernel_57115/529044046.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  result_dict = torch.load(os.path.join(f'{instance_dir}/ins_info', 'sem_seg.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0055\n",
      "0197\n",
      "Final predictions:\n",
      "Final predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:06<15:42,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions:\n",
      "0187\n",
      "Final predictions:\n",
      "Final predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:10<27:12,  5.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m ins_mask \u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_result_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m mask2d_view_list, mask_2d_bbox_correspondences, binary_masks_list \u001b[38;5;241m=\u001b[39m project_3d_to_2d(obj_xyz, top_k_masks, screen_coords, pc_depth)\n\u001b[0;32m---> 51\u001b[0m target_3d_masks \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_masks_and_calculate_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_views\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_masks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m assign_labels_to_masks(result_dict, target_3d_masks, num_views, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     53\u001b[0m scene_pcd \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_point_cloud(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/wan/Datasets/Test_scene/part_valid/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/points_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscene_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.ply\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Workplace-why/Part-SAM/utils/utils_3d.py:201\u001b[0m, in \u001b[0;36mprocess_masks_and_calculate_iou\u001b[0;34m(class_results, num_views, binary_masks_list, iou_threshold, normalized_threshold)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mask_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(binary_masks)):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sam_mask_idx, sam_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sam_masks):\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;66;03m# Calculate IoU for masks\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m         mask_iou \u001b[38;5;241m=\u001b[39m \u001b[43mbinaryMaskIOU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_masks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask_iou \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    203\u001b[0m             view_ious\u001b[38;5;241m.\u001b[39mappend((mask_idx, mask_iou))\n",
      "File \u001b[0;32m~/Workplace-why/Part-SAM/utils/utils_3d.py:75\u001b[0m, in \u001b[0;36mbinaryMaskIOU\u001b[0;34m(mask1, mask2)\u001b[0m\n\u001b[1;32m     73\u001b[0m     mask1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mask1)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask2, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 75\u001b[0m     mask2 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m intersection \u001b[38;5;241m=\u001b[39m (mask1 \u001b[38;5;241m*\u001b[39m mask2)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m intersection \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_dir = '/home/wan/Datasets/Test_scene/part_valid'\n",
    "project_path = '/home/wan/Workplace-why/PartScene'\n",
    "output_dir = '/home/wan/Workplace-why/PartScene/part_scene_results'\n",
    "final_masks_save_dir = os.path.join(project_path, 'part_scene_results')\n",
    "by_product_save_dir = 'part_scene_saved'\n",
    "ckpt_path = os.path.join(project_path, \"checkpoints/model.safetensors\")\n",
    "reversed_dict = {value: key for key, value in cls_dict.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for scene_id in tqdm(os.listdir('part_scene_saved')[:]):\n",
    "        print(scene_id)\n",
    "        if scene_id == '0055':\n",
    "                continue\n",
    "        scene_path = os.path.join(dataset_dir, scene_id, f'points_{scene_id}.ply')\n",
    "        mask_result_path = os.path.join(final_masks_save_dir, scene_id)\n",
    "        output_scene_dir = os.path.join(project_path, by_product_save_dir, scene_id)\n",
    "        mask_infos = load_prediction_data( f'{mask_result_path}/{scene_id}_summary.txt')\n",
    "        pred_part_mask_dir = os.path.join(mask_result_path, 'pred_part_mask')\n",
    "\n",
    "                # Check if the directory exists and remove it if necessary\n",
    "        if os.path.exists(pred_part_mask_dir):\n",
    "                        shutil.rmtree(pred_part_mask_dir)\n",
    "                # Create the directory\n",
    "        os.makedirs(pred_part_mask_dir)\n",
    "\n",
    "                # Check if the summary file exists\n",
    "        summary_file = os.path.join(mask_result_path, f'{scene_id}_part_summary.txt')\n",
    "        if os.path.exists(summary_file):\n",
    "                # Handle the case when the summary file exists (if needed)\n",
    "                os.remove(summary_file)\n",
    "\n",
    "        for idx,mask in enumerate(mask_infos):\n",
    "                ins_num = mask['prediction']\n",
    "                mask_file = mask['file']\n",
    "                instance_dir = os.path.join(output_scene_dir,str(idx))\n",
    "                top_k_masks,pc_depth,screen_coords,obj_xyz = load_instance_info(f'{instance_dir}/ins_info')\n",
    "                ins =reversed_dict[ins_num]\n",
    "                prompt = cls_part_dict[ins]\n",
    "                file_paths = glob.glob(os.path.join(f'{instance_dir}/rendered_img', '*'))\n",
    "                points_3d =[]\n",
    "                visible_pts_list = []\n",
    "                # Print all the files found\n",
    "                num_views = pc_depth.shape[0]\n",
    "                text_input = prompt\n",
    "                # load the segment result:\n",
    "                result_dict = torch.load(os.path.join(f'{instance_dir}/ins_info', 'sem_seg.pt'))\n",
    "                ins_mask =np.loadtxt(f'{mask_result_path}/{mask_file}').astype('bool')\n",
    "                mask2d_view_list, mask_2d_bbox_correspondences, binary_masks_list = project_3d_to_2d(obj_xyz, top_k_masks, screen_coords, pc_depth)\n",
    "                target_3d_masks = process_masks_and_calculate_iou(result_dict, num_views, binary_masks_list, 0,0.1)\n",
    "                final_predictions = assign_labels_to_masks(result_dict, target_3d_masks, num_views, N=2)\n",
    "                scene_pcd = o3d.io.read_point_cloud(f'/home/wan/Datasets/Test_scene/part_valid/{scene_id}/points_{scene_id}.ply')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                part_mask_after_process = process_mask_results(final_predictions,top_k_masks)\n",
    "                # save_mask_results(scene_id, part_mask_after_process, scene_pcd, ins_mask, ins, output_dir, part_label_v2)      \n",
    "        # break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1   0.15  0.2   0.25  0.3   0.35  0.4   0.45  0.5   0.55  0.6   0.65\n",
      " 0.7   0.75  0.8   0.85  0.9   0.24  0.324]\n",
      "0055\n",
      "0197\n",
      "0187\n",
      "0247\n",
      "0036\n",
      "0296\n",
      "0292\n",
      "0181\n",
      "0224\n",
      "0157\n",
      "0294\n",
      "0223\n",
      "0268\n",
      "0210\n",
      "0171\n",
      "0010\n",
      "0005\n",
      "0291\n",
      "0066\n",
      "0297\n",
      "0076\n",
      "0059\n",
      "0192\n",
      "0186\n",
      "0168\n",
      "0215\n",
      "0255\n",
      "0245\n",
      "0206\n",
      "0284\n",
      "0065\n",
      "0204\n",
      "0019\n",
      "0071\n",
      "0048\n",
      "0250\n",
      "0111\n",
      "0242\n",
      "0044\n",
      "0193\n",
      "0182\n",
      "0078\n",
      "0056\n",
      "0239\n",
      "0094\n",
      "0295\n",
      "0042\n",
      "0184\n",
      "0229\n",
      "0238\n",
      "0063\n",
      "0051\n",
      "0228\n",
      "0275\n",
      "0041\n",
      "0022\n",
      "0199\n",
      "0271\n",
      "0152\n",
      "0113\n",
      "0014\n",
      "0054\n",
      "0052\n",
      "0064\n",
      "0053\n",
      "0082\n",
      "0240\n",
      "0176\n",
      "0097\n",
      "0069\n",
      "0191\n",
      "0159\n",
      "0232\n",
      "0150\n",
      "0104\n",
      "0083\n",
      "0246\n",
      "0025\n",
      "0230\n",
      "0248\n",
      "0024\n",
      "0143\n",
      "0149\n",
      "0231\n",
      "0220\n",
      "0103\n",
      "0233\n",
      "0283\n",
      "0267\n",
      "0288\n",
      "0045\n",
      "0100\n",
      "0004\n",
      "0135\n",
      "0209\n",
      "0081\n",
      "0251\n",
      "0273\n",
      "0098\n",
      "0194\n",
      "0256\n",
      "0074\n",
      "0105\n",
      "0144\n",
      "0153\n",
      "0075\n",
      "0260\n",
      "0033\n",
      "0161\n",
      "0236\n",
      "0287\n",
      "0026\n",
      "0203\n",
      "0034\n",
      "0158\n",
      "0013\n",
      "0058\n",
      "0132\n",
      "0120\n",
      "0166\n",
      "0259\n",
      "0263\n",
      "0090\n",
      "0016\n",
      "0087\n",
      "0145\n",
      "0020\n",
      "0290\n",
      "0235\n",
      "0070\n",
      "0002\n",
      "0179\n",
      "0108\n",
      "0241\n",
      "0009\n",
      "0091\n",
      "0222\n",
      "0102\n",
      "0021\n",
      "0109\n",
      "0155\n",
      "0293\n",
      "0285\n",
      "0095\n",
      "0079\n",
      "0119\n",
      "0133\n",
      "0130\n",
      "0141\n",
      "0234\n",
      "0188\n",
      "0101\n",
      "0123\n",
      "0112\n",
      "0272\n",
      "0080\n",
      "0001\n",
      "0127\n",
      "0279\n",
      "0015\n",
      "0226\n",
      "0061\n",
      "0136\n",
      "0254\n",
      "0202\n",
      "0110\n",
      "0189\n",
      "0086\n",
      "0167\n",
      "0046\n",
      "0174\n",
      "0011\n",
      "0237\n",
      "0289\n",
      "0128\n",
      "0040\n",
      "0115\n",
      "0172\n",
      "0225\n",
      "0276\n",
      "0085\n",
      "0266\n",
      "0208\n",
      "0089\n",
      "0280\n",
      "0116\n",
      "0006\n",
      "0060\n",
      "0270\n",
      "0219\n",
      "0243\n",
      "0137\n",
      "0023\n",
      "0031\n",
      "0099\n",
      "0140\n",
      "0129\n",
      "0012\n",
      "0093\n",
      "0000\n",
      "0261\n",
      "0156\n",
      "0092\n",
      "0216\n",
      "0269\n",
      "0286\n",
      "0050\n",
      "0217\n",
      "0170\n",
      "0072\n",
      "0138\n",
      "0177\n",
      "0073\n",
      "0169\n",
      "0148\n",
      "0096\n",
      "0106\n",
      "0201\n",
      "0162\n",
      "0147\n",
      "0017\n",
      "0175\n",
      "0253\n",
      "0265\n",
      "0173\n",
      "0190\n",
      "0142\n",
      "0139\n",
      "0032\n",
      "0039\n",
      "0207\n",
      "0154\n",
      "0077\n",
      "0067\n",
      "0218\n",
      "0282\n",
      "0038\n",
      "0062\n",
      "0126\n",
      "0047\n",
      "0198\n",
      "0088\n",
      "0214\n",
      "0212\n",
      "0178\n",
      "0200\n",
      "0003\n",
      "0107\n",
      "0244\n",
      "0035\n",
      "0049\n",
      "0018\n",
      "0195\n",
      "0151\n",
      "0277\n",
      "0257\n",
      "0029\n",
      "0163\n",
      "0030\n",
      "0146\n",
      "0262\n",
      "0160\n",
      "0281\n",
      "0027\n",
      "0084\n",
      "0118\n",
      "0249\n",
      "0057\n",
      "0264\n",
      "0068\n",
      "0007\n",
      "0227\n",
      "0037\n",
      "0180\n",
      "0205\n",
      "0008\n",
      "0043\n",
      "0121\n",
      "0124\n",
      "0114\n",
      "0164\n",
      "0278\n",
      "0196\n",
      "0213\n",
      "0299\n",
      "0185\n",
      "0252\n",
      "0131\n",
      "0298\n",
      "0183\n",
      "0125\n",
      "0122\n",
      "0134\n",
      "0028\n",
      "0117\n",
      "0274\n",
      "0165\n",
      "0211\n",
      "0258\n",
      "0221\n",
      "evaluating 300 scans...\n",
      "/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0055.txt\n",
      "scans processed: 1/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0197.txt\n",
      "scans processed: 2/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0187.txt\n",
      "scans processed: 3/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0247.txt\n",
      "scans processed: 4/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0036.txt\n",
      "scans processed: 5/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0296.txt\n",
      "scans processed: 6/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0292.txt\n",
      "scans processed: 7/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0181.txt\n",
      "scans processed: 8/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0224.txt\n",
      "scans processed: 9/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0157.txt\n",
      "scans processed: 10/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0294.txt\n",
      "scans processed: 11/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0223.txt\n",
      "scans processed: 12/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0268.txt\n",
      "scans processed: 13/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0210.txt\n",
      "scans processed: 14/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0171.txt\n",
      "scans processed: 15/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0010.txt\n",
      "scans processed: 16/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0005.txt\n",
      "scans processed: 17"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57115/1865818175.py:396: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  pred_mask = np.not_equal(pred_mask, 0)\n",
      "/tmp/ipykernel_57115/1865818175.py:408: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  np.logical_and(bool_void, pred_mask)\n",
      "/tmp/ipykernel_57115/1865818175.py:417: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  np.logical_and(gt_ids == gt_inst[\"instance_id\"], pred_mask)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0291.txt\n",
      "scans processed: 18/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0066.txt\n",
      "scans processed: 19/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0297.txt\n",
      "scans processed: 20/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0076.txt\n",
      "scans processed: 21/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0059.txt\n",
      "scans processed: 22/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0192.txt\n",
      "scans processed: 23/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0186.txt\n",
      "scans processed: 24/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0168.txt\n",
      "scans processed: 25/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0215.txt\n",
      "scans processed: 26/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0255.txt\n",
      "scans processed: 27/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0245.txt\n",
      "scans processed: 28/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0206.txt\n",
      "scans processed: 29/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0284.txt\n",
      "scans processed: 30/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0065.txt\n",
      "scans processed: 31/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0204.txt\n",
      "scans processed: 32/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0019.txt\n",
      "scans processed: 33/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0071.txt\n",
      "scans processed: 34/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0048.txt\n",
      "scans processed: 35/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0250.txt\n",
      "scans processed: 36/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0111.txt\n",
      "scans processed: 37/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0242.txt\n",
      "scans processed: 38/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0044.txt\n",
      "scans processed: 39/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0193.txt\n",
      "scans processed: 40/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0182.txt\n",
      "scans processed: 41/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0078.txt\n",
      "scans processed: 42/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0056.txt\n",
      "scans processed: 43/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0239.txt\n",
      "scans processed: 44/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0094.txt\n",
      "scans processed: 45/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0295.txt\n",
      "scans processed: 46/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0042.txt\n",
      "scans processed: 47/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0184.txt\n",
      "scans processed: 48/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0229.txt\n",
      "scans processed: 49/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0238.txt\n",
      "scans processed: 50/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0063.txt\n",
      "scans processed: 51/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0051.txt\n",
      "scans processed: 52/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0228.txt\n",
      "scans processed: 53/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0275.txt\n",
      "scans processed: 54/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0041.txt\n",
      "scans processed: 55/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0022.txt\n",
      "scans processed: 56/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0199.txt\n",
      "scans processed: 57/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0271.txt\n",
      "scans processed: 58/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0152.txt\n",
      "scans processed: 59/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0113.txt\n",
      "scans processed: 60/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0014.txt\n",
      "scans processed: 61/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0054.txt\n",
      "scans processed: 62/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0052.txt\n",
      "scans processed: 63/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0064.txt\n",
      "scans processed: 64/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0053.txt\n",
      "scans processed: 65/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0082.txt\n",
      "scans processed: 66/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0240.txt\n",
      "scans processed: 67/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0176.txt\n",
      "scans processed: 68/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0097.txt\n",
      "scans processed: 69/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0069.txt\n",
      "scans processed: 70/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0191.txt\n",
      "scans processed: 71/home/wan/Datasets/Test_scene/part_valid_gt/gt_mask_0159.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 687\u001b[0m\n\u001b[1;32m    681\u001b[0m     evaluate(preds, gt_path,gt_label_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_final_result.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 681\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    674\u001b[0m     preds[scene_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_masks\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack(pred_masks)\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred_masks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_scores\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack(pred_scores))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred_masks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack(pred_class))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred_masks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[1;32m    678\u001b[0m     }\n\u001b[1;32m    680\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartScene\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 681\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgt_label_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_final_result.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 627\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(preds, gt_path, gt_label_path, output_file, dataset)\u001b[0m\n\u001b[1;32m    625\u001b[0m matches_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(gt_file)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# assign gt to predictions\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m gt2pred, pred2gt \u001b[38;5;241m=\u001b[39m \u001b[43massign_instances_for_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgt_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m matches[matches_key] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    629\u001b[0m matches[matches_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gt2pred\n",
      "Cell \u001b[0;32mIn[2], line 357\u001b[0m, in \u001b[0;36massign_instances_for_scan\u001b[0;34m(pred, gt_file, gt_dict)\u001b[0m\n\u001b[1;32m    355\u001b[0m pred_info \u001b[38;5;241m=\u001b[39m make_pred_info(pred)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     gt_ids \u001b[38;5;241m=\u001b[39m \u001b[43mutil_3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     util_3d\u001b[38;5;241m.\u001b[39mprint_error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munable to load \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m gt_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Workplace-why/Part-SAM/utils/eval_util.py:183\u001b[0m, in \u001b[0;36mload_ids\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ids\u001b[39m(filename):\n\u001b[1;32m    182\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m--> 183\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os, sys, argparse\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "from scipy import stats\n",
    "from utils.eval_util import *\n",
    "import \\\n",
    "    utils.eval_util as util_3d\n",
    "# import wandb\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_new_dataset_to_gt_instances(gt_ids, gt_dict, CLASS_LABELS, VALID_CLASS_IDS, ID_TO_LABEL):\n",
    "    # Create a mapping from gt_dict labels to standardized labels\n",
    "\n",
    "\n",
    "    # Initialize the gt_instances dictionary\n",
    "    gt_instances = {label: [] for label in CLASS_LABELS}\n",
    "\n",
    "    # Count the number of points for each instance\n",
    "    instance_point_counts = defaultdict(int)\n",
    "    for id in gt_ids:\n",
    "        instance_point_counts[id] += 1\n",
    "\n",
    "    # Process each unique instance\n",
    "    for instance_id, count in instance_point_counts.items():\n",
    "        # Get the label from gt_dict and map it to the standardized label\n",
    "        original_label = gt_dict[str(instance_id)]\n",
    "        standardized_label = util_3d.label_mapping.get(original_label, original_label)\n",
    "\n",
    "        # Find the corresponding label_id in VALID_CLASS_IDS\n",
    "        try:\n",
    "            label_id = VALID_CLASS_IDS[CLASS_LABELS.index(standardized_label)]\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Create the instance dictionary\n",
    "        instance_dict = {\n",
    "            'instance_id': int(instance_id),\n",
    "            'label_id': int(label_id),\n",
    "            'vert_count': int(count),\n",
    "            'med_dist': -1,\n",
    "            'dist_conf': 0.0\n",
    "        }\n",
    "\n",
    "        # Add the instance to the corresponding label in gt_instances\n",
    "        gt_instances[standardized_label].append(instance_dict)\n",
    "\n",
    "    return gt_instances\n",
    "\n",
    "\n",
    "def identify_void_areas(gt_ids, gt_dict, CLASS_LABELS, VALID_CLASS_IDS):\n",
    "    # Create a mapping from gt_dict labels to standardized labels\n",
    "\n",
    "\n",
    "    # Create a mapping from gt_ids to standardized class labels\n",
    "    id_to_standard_label = {\n",
    "        int(id): util_3d.label_mapping.get(label, label) \n",
    "        for id, label in gt_dict.items()\n",
    "    }\n",
    "\n",
    "    # Create a set of valid standardized labels\n",
    "    valid_labels = set(CLASS_LABELS)\n",
    "\n",
    "    # Function to check if a gt_id is valid\n",
    "    def is_valid(id):\n",
    "        return id_to_standard_label.get(id, '') in valid_labels\n",
    "\n",
    "    # Create boolean array indicating void areas\n",
    "    bool_void = np.vectorize(lambda x: not is_valid(x))(gt_ids)\n",
    "\n",
    "    return bool_void\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "def get_args():\n",
    "    \n",
    "    '''Command line arguments.'''\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='OpenIns3D evaluation')\n",
    "    parser.add_argument('--result_save', default=\"scannet_results\", type=str, help='Path of detection results')\n",
    "    parser.add_argument('--gt_path', default=\"data/processed/s3dis/instance_gt/Area_5\", help='Path of gt instance')\n",
    "    parser.add_argument('--dataset', default=\"part_scene\", help='dataset for evaluation, could be s3dis, scannet, stpls3d')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "args = get_args()\n",
    "dataset = args.dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ID_TO_LABEL = {}\n",
    "LABEL_TO_ID = {}\n",
    "for i in range(len(VALID_CLASS_IDS)):\n",
    "    LABEL_TO_ID[CLASS_LABELS[i]] = VALID_CLASS_IDS[i]\n",
    "    ID_TO_LABEL[VALID_CLASS_IDS[i]] = CLASS_LABELS[i]\n",
    "# ---------- Evaluation params ---------- #\n",
    "opt = {}\n",
    "opt[\"overlaps\"] = np.append(np.append(np.arange(0.1, 0.95, 0.05),0.24),0.324)\n",
    "\n",
    "print(opt[\"overlaps\"])\n",
    "# minimum region size for evaluation [verts]\n",
    "opt[\"min_region_sizes\"] = np.array([0])  # 100 for s3dis, scannet\n",
    "# distance thresholds [m]\n",
    "opt[\"distance_threshes\"] = np.array([float(\"inf\")])\n",
    "# distance confidences\n",
    "opt[\"distance_confs\"] = np.array([-float(\"inf\")])\n",
    "\n",
    "\n",
    "def evaluate_matches(matches):\n",
    "    overlaps = opt[\"overlaps\"]\n",
    "    min_region_sizes = [opt[\"min_region_sizes\"][0]]\n",
    "    dist_threshes = [opt[\"distance_threshes\"][0]]\n",
    "    dist_confs = [opt[\"distance_confs\"][0]]\n",
    "\n",
    "    # results: class x overlap\n",
    "    ap = np.zeros(\n",
    "        (len(dist_threshes), len(CLASS_LABELS), len(overlaps)), float\n",
    "    )\n",
    "    for di, (min_region_size, distance_thresh, distance_conf) in enumerate(\n",
    "        zip(min_region_sizes, dist_threshes, dist_confs)\n",
    "    ):\n",
    "        for oi, overlap_th in enumerate(overlaps):\n",
    "            pred_visited = {}\n",
    "            for m in matches:\n",
    "                for p in matches[m][\"pred\"]:\n",
    "                    for label_name in CLASS_LABELS:\n",
    "                        for p in matches[m][\"pred\"][label_name]:\n",
    "                            if \"uuid\" in p:\n",
    "                                pred_visited[p[\"uuid\"]] = False\n",
    "            for li, label_name in enumerate(CLASS_LABELS):\n",
    "                y_true = np.empty(0)\n",
    "                y_score = np.empty(0)\n",
    "                hard_false_negatives = 0\n",
    "                has_gt = False\n",
    "                has_pred = False\n",
    "                for m in matches:\n",
    "                    pred_instances = matches[m][\"pred\"][label_name]\n",
    "                    gt_instances = matches[m][\"gt\"][label_name]\n",
    "                    # filter groups in ground truth\n",
    "                    gt_instances = [\n",
    "                        gt\n",
    "                        for gt in gt_instances\n",
    "                        # if gt[\"instance_id\"] >= 1000\n",
    "                        if gt[\"vert_count\"] >= min_region_size\n",
    "                        and gt[\"med_dist\"] <= distance_thresh\n",
    "                        and gt[\"dist_conf\"] >= distance_conf\n",
    "                    ]\n",
    "                    if gt_instances:\n",
    "                        has_gt = True\n",
    "                    if pred_instances:\n",
    "                        has_pred = True\n",
    "\n",
    "                    cur_true = np.ones(len(gt_instances))\n",
    "                    cur_score = np.ones(len(gt_instances)) * (-float(\"inf\"))\n",
    "                    cur_match = np.zeros(len(gt_instances), dtype=bool)\n",
    "                    # collect matches\n",
    "                    for (gti, gt) in enumerate(gt_instances):\n",
    "                        found_match = False\n",
    "                        num_pred = len(gt[\"matched_pred\"])\n",
    "                        for pred in gt[\"matched_pred\"]:\n",
    "                            # greedy assignments\n",
    "                            if pred_visited[pred[\"uuid\"]]:\n",
    "                                continue\n",
    "                            overlap = float(pred[\"intersection\"]) / (\n",
    "                                gt[\"vert_count\"]\n",
    "                                + pred[\"vert_count\"]\n",
    "                                - pred[\"intersection\"]\n",
    "                            )\n",
    "                            if overlap > overlap_th:\n",
    "                                confidence = pred[\"confidence\"]\n",
    "                                # if already have a prediction for this gt,\n",
    "                                # the prediction with the lower score is automatically a false positive\n",
    "                                if cur_match[gti]:\n",
    "                                    max_score = max(cur_score[gti], confidence)\n",
    "                                    min_score = min(cur_score[gti], confidence)\n",
    "                                    cur_score[gti] = max_score\n",
    "                                    # append false positive\n",
    "                                    cur_true = np.append(cur_true, 0)\n",
    "                                    cur_score = np.append(cur_score, min_score)\n",
    "                                    cur_match = np.append(cur_match, True)\n",
    "                                # otherwise set score\n",
    "                                else:\n",
    "                                    found_match = True\n",
    "                                    cur_match[gti] = True\n",
    "                                    cur_score[gti] = confidence\n",
    "                                    pred_visited[pred[\"uuid\"]] = True\n",
    "                        if not found_match:\n",
    "                            hard_false_negatives += 1\n",
    "                    # remove non-matched ground truth instances\n",
    "                    cur_true = cur_true[cur_match == True]\n",
    "                    cur_score = cur_score[cur_match == True]\n",
    "\n",
    "                    # collect non-matched predictions as false positive\n",
    "                    for pred in pred_instances:\n",
    "                        found_gt = False\n",
    "                        for gt in pred[\"matched_gt\"]:\n",
    "                            overlap = float(gt[\"intersection\"]) / (\n",
    "                                gt[\"vert_count\"]\n",
    "                                + pred[\"vert_count\"]\n",
    "                                - gt[\"intersection\"]\n",
    "                            )\n",
    "                            if overlap > overlap_th:\n",
    "                                found_gt = True\n",
    "                                break\n",
    "                        if not found_gt:\n",
    "                            num_ignore = pred[\"void_intersection\"]\n",
    "                            for gt in pred[\"matched_gt\"]:\n",
    "                                # group?\n",
    "                                if gt[\"instance_id\"] < 1000:\n",
    "                                    num_ignore += gt[\"intersection\"]\n",
    "                                # small ground truth instances\n",
    "                                if (\n",
    "                                    gt[\"vert_count\"] < min_region_size\n",
    "                                    or gt[\"med_dist\"] > distance_thresh\n",
    "                                    or gt[\"dist_conf\"] < distance_conf\n",
    "                                ):\n",
    "                                    num_ignore += gt[\"intersection\"]\n",
    "                            proportion_ignore = (\n",
    "                                float(num_ignore) / pred[\"vert_count\"]\n",
    "                            )\n",
    "                            # if not ignored append false positive\n",
    "                            if proportion_ignore <= overlap_th:\n",
    "                                cur_true = np.append(cur_true, 0)\n",
    "                                confidence = pred[\"confidence\"]\n",
    "                                cur_score = np.append(cur_score, confidence)\n",
    "\n",
    "                    # append to overall results\n",
    "                    y_true = np.append(y_true, cur_true)\n",
    "                    y_score = np.append(y_score, cur_score)\n",
    "\n",
    "                # compute average precision\n",
    "                if has_gt and has_pred:\n",
    "                    # compute precision recall curve first\n",
    "\n",
    "                    # sorting and cumsum\n",
    "                    score_arg_sort = np.argsort(y_score)\n",
    "                    y_score_sorted = y_score[score_arg_sort]\n",
    "                    y_true_sorted = y_true[score_arg_sort]\n",
    "                    y_true_sorted_cumsum = np.cumsum(y_true_sorted)\n",
    "\n",
    "                    # unique thresholds\n",
    "                    (thresholds, unique_indices) = np.unique(\n",
    "                        y_score_sorted, return_index=True\n",
    "                    )\n",
    "                    num_prec_recall = len(unique_indices) + 1\n",
    "\n",
    "                    # prepare precision recall\n",
    "                    num_examples = len(y_score_sorted)\n",
    "                    # https://github.com/ScanNet/ScanNet/pull/26\n",
    "                    # all predictions are non-matched but also all of them are ignored and not counted as FP\n",
    "                    # y_true_sorted_cumsum is empty\n",
    "                    # num_true_examples = y_true_sorted_cumsum[-1]\n",
    "                    num_true_examples = (\n",
    "                        y_true_sorted_cumsum[-1]\n",
    "                        if len(y_true_sorted_cumsum) > 0\n",
    "                        else 0\n",
    "                    )\n",
    "                    precision = np.zeros(num_prec_recall)\n",
    "                    recall = np.zeros(num_prec_recall)\n",
    "\n",
    "                    # deal with the first point\n",
    "                    y_true_sorted_cumsum = np.append(y_true_sorted_cumsum, 0)\n",
    "                    # deal with remaining\n",
    "                    for idx_res, idx_scores in enumerate(unique_indices):\n",
    "                        cumsum = y_true_sorted_cumsum[idx_scores - 1]\n",
    "                        tp = num_true_examples - cumsum\n",
    "                        fp = num_examples - idx_scores - tp\n",
    "                        fn = cumsum + hard_false_negatives\n",
    "                        p = float(tp) / (tp + fp)\n",
    "                        r = float(tp) / (tp + fn)\n",
    "                        precision[idx_res] = p\n",
    "                        recall[idx_res] = r\n",
    "\n",
    "                    # first point in curve is artificial\n",
    "                    precision[-1] = 1.0\n",
    "                    recall[-1] = 0.0\n",
    "\n",
    "                    # compute average of precision-recall curve\n",
    "                    recall_for_conv = np.copy(recall)\n",
    "                    recall_for_conv = np.append(\n",
    "                        recall_for_conv[0], recall_for_conv\n",
    "                    )\n",
    "                    recall_for_conv = np.append(recall_for_conv, 0.0)\n",
    "\n",
    "                    stepWidths = np.convolve(\n",
    "                        recall_for_conv, [-0.5, 0, 0.5], \"valid\"\n",
    "                    )\n",
    "                    # integrate is now simply a dot product\n",
    "                    ap_current = np.dot(precision, stepWidths)\n",
    "\n",
    "                elif has_gt:\n",
    "                    ap_current = 0.0\n",
    "                else:\n",
    "                    ap_current = float(\"nan\")\n",
    "                ap[di, li, oi] = ap_current\n",
    "    return ap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_pred_info(pred: dict):\n",
    "    # pred = {'pred_scores' = 100, 'pred_classes' = 100 'pred_masks' = Nx100}\n",
    "    pred_info = {}\n",
    "    assert (\n",
    "        pred[\"pred_classes\"].shape[0]\n",
    "        == pred[\"pred_scores\"].shape[0]\n",
    "        == pred[\"pred_masks\"].shape[1]\n",
    "    )\n",
    "    for i in range(len(pred[\"pred_classes\"])):\n",
    "        info = {}\n",
    "        info[\"label_id\"] = pred[\"pred_classes\"][i]\n",
    "        info[\"conf\"] = pred[\"pred_scores\"][i]\n",
    "        info[\"mask\"] = pred[\"pred_masks\"][:, i]\n",
    "        pred_info[uuid4()] = info  # we later need to identify these objects\n",
    "    return pred_info\n",
    "\n",
    "\n",
    "def assign_instances_for_scan(pred: dict, gt_file: str, gt_dict: dict):\n",
    "    pred_info = make_pred_info(pred)\n",
    "    try:\n",
    "        gt_ids = util_3d.load_ids(gt_file)\n",
    "    except Exception as e:\n",
    "        util_3d.print_error(\"unable to load \" + gt_file + \": \" + str(e))\n",
    "\n",
    "    #load the gt dict\n",
    "\n",
    "    # get gt instances\n",
    "\n",
    "    # # breakpoint()\n",
    "    # gt_instances = util_3d.get_instances(\n",
    "    #     gt_ids, VALID_CLASS_IDS, CLASS_LABELS, ID_TO_LABEL\n",
    "    # )\n",
    "\n",
    "    gt_instances = convert_new_dataset_to_gt_instances(gt_ids, gt_dict, CLASS_LABELS, VALID_CLASS_IDS, ID_TO_LABEL)\n",
    "\n",
    "    # breakpoint()\n",
    "    # associate\n",
    "    gt2pred = deepcopy(gt_instances)\n",
    "    for label in gt2pred:\n",
    "        for gt in gt2pred[label]:\n",
    "            gt[\"matched_pred\"] = []\n",
    "    pred2gt = {}\n",
    "    for label in CLASS_LABELS:\n",
    "        pred2gt[label] = []\n",
    "    num_pred_instances = 0\n",
    "    # mask of void labels in the groundtruth\n",
    "    # breakpoint()\n",
    "    bool_void = identify_void_areas(gt_ids, gt_dict, CLASS_LABELS, VALID_CLASS_IDS)\n",
    "    # go thru all prediction masks\n",
    "    for uuid in pred_info:\n",
    "        label_id = int(pred_info[uuid][\"label_id\"])\n",
    "        conf = pred_info[uuid][\"conf\"]\n",
    "        if not label_id in ID_TO_LABEL:\n",
    "            continue\n",
    "        label_name = ID_TO_LABEL[label_id]\n",
    "        # read the mask\n",
    "        pred_mask = pred_info[uuid][\"mask\"]\n",
    "        assert len(pred_mask) == len(gt_ids)\n",
    "        # convert to binary\n",
    "        pred_mask = np.not_equal(pred_mask, 0)\n",
    "        num = np.count_nonzero(pred_mask)\n",
    "        if num < opt[\"min_region_sizes\"][0]:\n",
    "            continue  # skip if empty\n",
    "\n",
    "        pred_instance = {}\n",
    "        pred_instance[\"uuid\"] = uuid\n",
    "        pred_instance[\"pred_id\"] = num_pred_instances\n",
    "        pred_instance[\"label_id\"] = label_id\n",
    "        pred_instance[\"vert_count\"] = num\n",
    "        pred_instance[\"confidence\"] = conf\n",
    "        pred_instance[\"void_intersection\"] = np.count_nonzero(\n",
    "            np.logical_and(bool_void, pred_mask)\n",
    "        )\n",
    "\n",
    "        # matched gt instances\n",
    "        matched_gt = []\n",
    "        # go thru all gt instances with matching label\n",
    "\n",
    "        for (gt_num, gt_inst) in enumerate(gt2pred[label_name]):\n",
    "            intersection = np.count_nonzero(\n",
    "                np.logical_and(gt_ids == gt_inst[\"instance_id\"], pred_mask)\n",
    "            )\n",
    "            if intersection > 0:\n",
    "                gt_copy = gt_inst.copy()\n",
    "                pred_copy = pred_instance.copy()\n",
    "                gt_copy[\"intersection\"] = intersection\n",
    "                pred_copy[\"intersection\"] = intersection\n",
    "                matched_gt.append(gt_copy)\n",
    "                gt2pred[label_name][gt_num][\"matched_pred\"].append(pred_copy)\n",
    "        pred_instance[\"matched_gt\"] = matched_gt\n",
    "        num_pred_instances += 1\n",
    "        pred2gt[label_name].append(pred_instance)\n",
    "\n",
    "    return gt2pred, pred2gt\n",
    "\n",
    "\n",
    "def print_results(avgs):\n",
    "    # wandb.login(key='d27f3b3e72d749fb99315e0e86c6b36b6e23617e')\n",
    "    # wandb.init(project=\"3D Open World Understanding}\",\n",
    "    #                    name='OpenIns3D')\n",
    "    sep = \"\"\n",
    "    col1 = \":\"\n",
    "    lineLen = 64\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"#\" * lineLen)\n",
    "    line = \"\"\n",
    "    line += \"{:<15}\".format(\"what\") + sep + col1\n",
    "    line += \"{:>15}\".format(\"AP\") + sep\n",
    "    line += \"{:>15}\".format(\"AP_50%\") + sep\n",
    "    line += \"{:>15}\".format(\"AP_25%\") + sep\n",
    "    print(line)\n",
    "    print(\"#\" * lineLen)\n",
    "    columns = ['Class','AP','AP_50%','AP_25%']\n",
    "    # result_table = wandb.Table(columns=columns)\n",
    "    for (li, label_name) in enumerate(CLASS_LABELS):\n",
    "        ap_avg = avgs[\"classes\"][label_name][\"ap\"]\n",
    "        ap_50o = avgs[\"classes\"][label_name][\"ap50%\"]\n",
    "        ap_25o = avgs[\"classes\"][label_name][\"ap25%\"]\n",
    "        # line = \"{:<15}\".format(label_name) + sep + col1\n",
    "        # line += sep + \"{:>15.3f}\".format(ap_avg) + sep\n",
    "        # line += sep + \"{:>15.3f}\".format(ap_50o) + sep\n",
    "        # line += sep + \"{:>15.3f}\".format(ap_25o) + sep\n",
    "        # print(line)\n",
    "        # result_table.add_data(label_name, ap_avg, ap_50o, ap_25o)\n",
    "\n",
    "    all_ap_avg = avgs[\"all_ap\"]\n",
    "    all_ap_50o = avgs[\"all_ap_50%\"]\n",
    "    all_ap_25o = avgs[\"all_ap_25%\"]\n",
    "    # wandb.log({\"AP\":all_ap_avg,\"AP_50\":all_ap_50o,\"AP_25\":all_ap_25o})\n",
    "    # wandb.log({\"Class_AP\":result_table})\n",
    "    print(\"-\" * lineLen)\n",
    "    line = \"{:<15}\".format(\"average\") + sep + col1\n",
    "    line += \"{:>15.3f}\".format(all_ap_avg) + sep\n",
    "    line += \"{:>15.3f}\".format(all_ap_50o) + sep\n",
    "    line += \"{:>15.3f}\".format(all_ap_25o) + sep\n",
    "    print(line)\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    preds: dict, gt_path: str, gt_label_path: str, output_file: str, dataset: str = \"scannet\"\n",
    "):\n",
    "    global CLASS_LABELS\n",
    "    global VALID_CLASS_IDS\n",
    "    global ID_TO_LABEL\n",
    "    global LABEL_TO_ID\n",
    "    global opt\n",
    "\n",
    "    total_true = 0\n",
    "    total_seen = 0\n",
    "    NUM_CLASSES = len(VALID_CLASS_IDS)\n",
    "\n",
    "    true_positive_classes = np.zeros(NUM_CLASSES)\n",
    "    positive_classes = np.zeros(NUM_CLASSES)\n",
    "    gt_classes = np.zeros(NUM_CLASSES)\n",
    "\n",
    "    # precision & recall\n",
    "    total_gt_ins = np.zeros(NUM_CLASSES)\n",
    "    at = 0.5\n",
    "    tpsins = [[] for _ in range(NUM_CLASSES)]\n",
    "    fpsins = [[] for _ in range(NUM_CLASSES)]\n",
    "    # mucov and mwcov\n",
    "    all_mean_cov = [[] for _ in range(NUM_CLASSES)]\n",
    "    all_mean_weighted_cov = [[] for _ in range(NUM_CLASSES)]\n",
    "\n",
    "    print(\"evaluating\", len(preds), \"scans...\")\n",
    "    matches = {}\n",
    "    for i, (k, v) in enumerate(preds.items()):\n",
    "        gt_file = os.path.join(gt_path, f\"gt_mask_{k}.txt\")\n",
    "        label_dict_path = os.path.join(gt_label_path,f'id2part_r{k}.json')\n",
    "        gt_label = util_3d.load_json(label_dict_path)\n",
    "        print(gt_file)\n",
    "        if not os.path.isfile(gt_file):\n",
    "            util_3d.print_error(\n",
    "                \"Scan {} does not match any gt file\".format(k), user_fault=True\n",
    "            )\n",
    "\n",
    "        if dataset == \"s3dis\":\n",
    "            gt_ids = util_3d.load_ids(gt_file)\n",
    "            gt_sem = (gt_ids // 1000) - 1\n",
    "            gt_ins = gt_ids - (gt_ids // 1000) * 1000\n",
    "\n",
    "            # pred_sem = v['pred_classes'] - 1\n",
    "            pred_sem = np.zeros(v[\"pred_masks\"].shape[0], dtype=np.int)\n",
    "            # TODO CONTINUE HERE!!!!!!!!!!!!!\n",
    "            pred_ins = np.zeros(v[\"pred_masks\"].shape[0], dtype=np.int)\n",
    "\n",
    "            for inst_id in reversed(range(v[\"pred_masks\"].shape[1])):\n",
    "                point_ids = np.argwhere(v[\"pred_masks\"][:, inst_id] == 1.0)[\n",
    "                    :, 0\n",
    "                ]\n",
    "                pred_ins[point_ids] = inst_id + 1\n",
    "                pred_sem[point_ids] = v[\"pred_classes\"][inst_id] - 1\n",
    "\n",
    "            # semantic acc\n",
    "            total_true += np.sum(pred_sem == gt_sem)\n",
    "            total_seen += pred_sem.shape[0]\n",
    "\n",
    "            # TODO PARALLELIZ THIS!!!!!!!\n",
    "            # pn semantic mIoU\n",
    "            \"\"\"\n",
    "            for j in range(gt_sem.shape[0]):\n",
    "                gt_l = int(gt_sem[j])\n",
    "                pred_l = int(pred_sem[j])\n",
    "                gt_classes[gt_l] += 1\n",
    "                positive_classes[pred_l] += 1\n",
    "                true_positive_classes[gt_l] += int(gt_l == pred_l)\n",
    "            \"\"\"\n",
    "\n",
    "            uniq, counts = np.unique(pred_sem, return_counts=True)\n",
    "            positive_classes[uniq] += counts\n",
    "\n",
    "            uniq, counts = np.unique(gt_sem, return_counts=True)\n",
    "            gt_classes[uniq] += counts\n",
    "\n",
    "            uniq, counts = np.unique(\n",
    "                gt_sem[pred_sem == gt_sem], return_counts=True\n",
    "            )\n",
    "            true_positive_classes[uniq] += counts\n",
    "\n",
    "            # instance\n",
    "            un = np.unique(pred_ins)\n",
    "            pts_in_pred = [[] for _ in range(NUM_CLASSES)]\n",
    "            for ig, g in enumerate(un):  # each object in prediction\n",
    "                if g == -1:\n",
    "                    continue\n",
    "                tmp = pred_ins == g\n",
    "                sem_seg_i = int(stats.mode(pred_sem[tmp])[0])\n",
    "                pts_in_pred[sem_seg_i] += [tmp]\n",
    "\n",
    "            un = np.unique(gt_ins)\n",
    "            pts_in_gt = [[] for _ in range(NUM_CLASSES)]\n",
    "            for ig, g in enumerate(un):\n",
    "                tmp = gt_ins == g\n",
    "                sem_seg_i = int(stats.mode(gt_sem[tmp])[0])\n",
    "                pts_in_gt[sem_seg_i] += [tmp]\n",
    "\n",
    "            # instance mucov & mwcov\n",
    "            for i_sem in range(NUM_CLASSES):\n",
    "                sum_cov = 0\n",
    "                mean_cov = 0\n",
    "                mean_weighted_cov = 0\n",
    "                num_gt_point = 0\n",
    "                for ig, ins_gt in enumerate(pts_in_gt[i_sem]):\n",
    "                    ovmax = 0.0\n",
    "                    num_ins_gt_point = np.sum(ins_gt)\n",
    "                    num_gt_point += num_ins_gt_point\n",
    "                    for ip, ins_pred in enumerate(pts_in_pred[i_sem]):\n",
    "                        union = ins_pred | ins_gt\n",
    "                        intersect = ins_pred & ins_gt\n",
    "                        iou = float(np.sum(intersect)) / np.sum(union)\n",
    "\n",
    "                        if iou > ovmax:\n",
    "                            ovmax = iou\n",
    "                            ipmax = ip\n",
    "\n",
    "                    sum_cov += ovmax\n",
    "                    mean_weighted_cov += ovmax * num_ins_gt_point\n",
    "\n",
    "                if len(pts_in_gt[i_sem]) != 0:\n",
    "                    mean_cov = sum_cov / len(pts_in_gt[i_sem])\n",
    "                    all_mean_cov[i_sem].append(mean_cov)\n",
    "\n",
    "                    mean_weighted_cov /= num_gt_point\n",
    "                    all_mean_weighted_cov[i_sem].append(mean_weighted_cov)\n",
    "\n",
    "\n",
    "        matches_key = os.path.abspath(gt_file)\n",
    "        # assign gt to predictions\n",
    "        gt2pred, pred2gt = assign_instances_for_scan(v, gt_file,gt_label)\n",
    "        matches[matches_key] = {}\n",
    "        matches[matches_key][\"gt\"] = gt2pred\n",
    "        matches[matches_key][\"pred\"] = pred2gt\n",
    "        sys.stdout.write(\"\\rscans processed: {}\".format(i + 1))\n",
    "        sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    ap_scores = evaluate_matches(matches)\n",
    "    avgs = compute_averages(ap_scores)\n",
    "\n",
    "    # print\n",
    "    print_results(avgs)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    pred_dir = 'part_scene_results'\n",
    "\n",
    "    gt_path = '/home/wan/Datasets/Test_scene/part_valid_gt'\n",
    "    gt_label_path = '/home/wan/Datasets/Test_scene/id2part_valid_gt'\n",
    "    finished_scene_path = glob.glob(pred_dir+\"/*\")\n",
    "    finished_scene = [scene.split(\"/\")[-1] for scene in finished_scene_path]\n",
    "    \n",
    "    preds = {}\n",
    "    #part_scene_results/0001/0001_part_summary.txt\n",
    "    for scene_name in finished_scene[:]:\n",
    "        print(scene_name)\n",
    "        file_path = os.path.join(pred_dir, scene_name, scene_name + '_part_summary.txt')  # {SCENE_ID}.txt file\n",
    "        scene_pred_mask_list = np.loadtxt(file_path, dtype=str)  # (num_masks, 2)\n",
    "        scene_pred_mask_list = scene_pred_mask_list.reshape(-1,3)\n",
    "        assert scene_pred_mask_list.shape[1] == 3, f'{scene_name} Each line should have 2 values: instance mask path and confidence score.'\n",
    "\n",
    "        pred_masks = []\n",
    "        pred_scores = []\n",
    "        pred_class = []\n",
    "\n",
    "        for mask_path, prediction, conf_score in scene_pred_mask_list: \n",
    "            # Read mask and confidence score for each instance mask\n",
    "            pred_mask = np.loadtxt(os.path.join(pred_dir, scene_name, mask_path), dtype=int) # Values: 0 for the background, 1 for the instance\n",
    "            pred_masks.append(pred_mask)\n",
    "            pred_scores.append(float(conf_score))\n",
    "            pred_class.append(int(prediction))\n",
    "\n",
    "        assert len(pred_masks) == len(pred_scores) == len(pred_class), f'{scene_name}Number of masks and confidence scores should be the same.'\n",
    "\n",
    "        # Aggregate masks and scores for each scene - pred_class is always 1 (we only have one semantic class, 'object', referring to the query object)\n",
    "        preds[scene_name] = {\n",
    "            'pred_masks': torch.from_numpy(np.vstack(pred_masks).T) if len(pred_masks) > 0 else np.zeros((1, 1)),\n",
    "            'pred_scores': torch.from_numpy(np.vstack(pred_scores)).squeeze(0) if len(pred_masks) > 0 else np.zeros(1),\n",
    "            'pred_classes': torch.from_numpy(np.vstack(pred_class)).squeeze(0) if len(pred_masks) > 0 else np.ones(1, dtype=np.int64)*255\n",
    "        }\n",
    "\n",
    "    evaluate(preds, gt_path,gt_label_path, f\"./{dataset}_final_result.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
