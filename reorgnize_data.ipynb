{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pdb\n",
    "import trimesh\n",
    "from utils.trans import *\n",
    "from utils.utils import *\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import torch\n",
    "import open3d as o3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# base_path = '/home/lidosan/Datasets/datasets--ShapeNet--PartNet-archive/blobs/data_v0/'\n",
    "\n",
    "# directories = os.listdir(base_path)\n",
    "# obj_correspondence = {}\n",
    "\n",
    "# for obj_id in directories:\n",
    "#     dir_path = os.path.join(base_path, obj_id)\n",
    "#     meta_file = os.path.join(dir_path, 'meta.json')\n",
    "#     parts_render_file = os.path.join(dir_path, 'parts_render', '0.png')\n",
    "    \n",
    "#     if os.path.isfile(meta_file) and os.path.isfile(parts_render_file):\n",
    "#         with open(meta_file, 'r') as f:\n",
    "#             meta_data = json.load(f)\n",
    "        \n",
    "#         category = meta_data.get('model_cat')\n",
    "        \n",
    "#         if category not in obj_correspondence:\n",
    "#             obj_correspondence[category] = {}\n",
    "        \n",
    "#         obj_correspondence[category][obj_id] = meta_file\n",
    "\n",
    "# # Write the obj_correspondence dictionary to a JSON file\n",
    "# output_json_path = os.path.join('', 'obj_correspondence.json')\n",
    "# with open(output_json_path, 'w') as outfile:\n",
    "#     json.dump(obj_correspondence, outfile, indent=4)\n",
    "\n",
    "# print(\"Organizing and copying completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "base_path = '/home/lidosan/Datasets/datasets--ShapeNet--PartNet-archive/blobs/data_v0/'\n",
    "obj_ids = [\"46531\", \"12159\", \"2554\", \"2355\", \"18233\", \"18277\", \"22981\", \"45187\",\"9067\",\"3512\",\"3810\",\"15719\",\"10207\",\"6490\",\"8795\",'3444','4739','7688']\n",
    "\n",
    "directories = os.listdir(base_path)\n",
    "obj_correspondence = {}\n",
    "\n",
    "for obj_id in obj_ids:\n",
    "    dir_path = os.path.join(base_path, obj_id)\n",
    "    meta_file = os.path.join(dir_path, 'meta.json')\n",
    "    parts_render_file = os.path.join(dir_path, 'parts_render', '0.png')\n",
    "    \n",
    "    if os.path.isfile(meta_file) and os.path.isfile(parts_render_file):\n",
    "        with open(meta_file, 'r') as f:\n",
    "            meta_data = json.load(f)\n",
    "        \n",
    "        category = meta_data.get('model_cat')\n",
    "        \n",
    "        if category not in obj_correspondence:\n",
    "            obj_correspondence[category] = {}\n",
    "        \n",
    "        obj_correspondence[category][obj_id] = meta_file\n",
    "\n",
    "# Write the obj_correspondence dictionary to a JSON file\n",
    "output_json_path = os.path.join('', 'obj_correspondence_tiny.json')\n",
    "with open(output_json_path, 'w') as outfile:\n",
    "    json.dump(obj_correspondence, outfile, indent=4)\n",
    "\n",
    "print(\"Organizing and copying completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_the_label(category,obj_structure):\n",
    "    parts = extract_parts_v2(obj_structure)\n",
    "    #TODO merge and extract the sub category\n",
    "    if category in ['StorageFurniture','Table','Table','Bed','Earphone','Faucet']:\n",
    "   \n",
    "        pass\n",
    "    if category in ['StorageFurniture']:\n",
    "            #TODO Select the cabniet\n",
    "            sub_category = parts[0][0].split('_')[1].strip()\n",
    "            if sub_category == 'Cabinet':\n",
    "                for part in parts:\n",
    "                    print(part)\n",
    "                    if 'Sink' in part[0]:\n",
    "                        sub_category = 'Sink Cabinet'\n",
    "                        print(sub_category)\n",
    "                        break\n",
    "                    elif 'Mirror' in part[0]:\n",
    "                        sub_category = 'Mirror Cabinet'\n",
    "                        print(sub_category)\n",
    "                        break\n",
    "                    elif 'Book' in part[1]:\n",
    "                        sub_category = 'Book Shelf'\n",
    "                        print(sub_category)\n",
    "                        break\n",
    "\n",
    "    if category in ['Chair']:\n",
    "            # Extract parts with labels and object files\n",
    "            for part in parts:\n",
    "                if \"Sofa-style\" in part[0]:\n",
    "                    sub_category = 'Sofa Chair'\n",
    "                    print(sub_category)\n",
    "                    break\n",
    "                elif 'Star' in part[0]:\n",
    "                    sub_category = 'Office Chair'\n",
    "                    print(sub_category)\n",
    "                    break\n",
    "                sub_category = 'Regular Chair'\n",
    "    return category,sub_category\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trimesh.load('scene_2b_colored/mesh_trans/Bed_caa_12159.obj').vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# # Load your mesh\n",
    "# mesh = trimesh.load('path/to/your/mesh.obj')\n",
    "\n",
    "# Generate random labels for each vertex\n",
    "vertex_labels = np.random.randint(0, 1000, size=len(mesh.vertices))\n",
    "\n",
    "# Function to downsample vertices\n",
    "def downsample_mesh(mesh, target_num_vertices):\n",
    "    # Use Trimesh's built-in simplification (decimation) method\n",
    "    # The function simplifies the mesh to a target number of faces, not vertices\n",
    "    # So we need to estimate the number of faces that would approximately give us the target number of vertices\n",
    "    target_num_faces = int(target_num_vertices * (len(mesh.faces) / len(mesh.vertices)))\n",
    "    \n",
    "    # Simplify the mesh\n",
    "    simplified_mesh = mesh.simplify_quadratic_decimation(target_num_faces)\n",
    "    \n",
    "    # Return the simplified mesh\n",
    "    return simplified_mesh\n",
    "\n",
    "# # Target number of vertices\n",
    "# target_num_vertices = 1000  # Set your desired number of vertices\n",
    "\n",
    "# # Downsample the mesh\n",
    "# simplified_mesh = downsample_mesh(mesh, target_num_vertices)\n",
    "\n",
    "# # Create a KDTree for the original and simplified vertices\n",
    "# original_vertices = mesh.vertices\n",
    "# simplified_vertices = simplified_mesh.vertices\n",
    "\n",
    "# tree = cKDTree(original_vertices)\n",
    "\n",
    "# # Find the nearest original vertex for each simplified vertex\n",
    "# _, indices = tree.query(simplified_vertices)\n",
    "\n",
    "# # Map the labels to the simplified vertices\n",
    "# simplified_vertex_labels = vertex_labels[indices]\n",
    "\n",
    "# # Now `simplified_mesh` has the downsampled vertices\n",
    "# # and `simplified_vertex_labels` contains the labels for the simplified vertices\n",
    "\n",
    "# # Save the simplified mesh if needed\n",
    "# simplified_mesh.export('path/to/simplified_mesh.obj')\n",
    "\n",
    "# # Print out the simplified vertices and their labels\n",
    "# for i, vertex in enumerate(simplified_vertices):\n",
    "#     print(f\"Vertex: {vertex}, Label: {simplified_vertex_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_mesh(mesh, target_num_vertices):\n",
    "    # Use Trimesh's built-in simplification (decimation) method\n",
    "    # The function simplifies the mesh to a target number of faces, not vertices\n",
    "    # So we need to estimate the number of faces that would approximately give us the target number of vertices\n",
    "    target_num_faces = int(target_num_vertices * (len(mesh.faces) / len(mesh.vertices)))\n",
    "    \n",
    "    # Simplify the mesh\n",
    "    simplified_mesh = mesh.simplify_quadratic_decimation(target_num_faces)\n",
    "    \n",
    "    # Return the simplified mesh\n",
    "    return simplified_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_base_dir = f'/home/lidosan/Datasets/datasets--ShapeNet--PartNet-archive/blobs/data_v0/46531'\n",
    "obj_structure = load_json(os.path.join(obj_base_dir, 'result.json'))\n",
    "model_cat = load_json(os.path.join(obj_base_dir, 'meta.json'))['model_cat']\n",
    "parts_render_img = os.path.join(obj_base_dir, 'parts_render', '0.png')\n",
    "# Extract parts with labels and object files\n",
    "parts = extract_parts_v2(obj_structure)\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parts2obj_uv(obj_id, parts, label_to_index):\n",
    "    obj_base_dir = f'/home/lidosan/Datasets/datasets--ShapeNet--PartNet-archive/blobs/data_v0/{obj_id}/objs'\n",
    "    \n",
    "    combined_mesh_list = []\n",
    "    combined_vertices = []\n",
    "    combined_labels = []\n",
    "    obbs_dict = {}\n",
    "    count_dict = {}\n",
    "    unique_labels = {label for label, _ in parts}\n",
    "\n",
    "    label_counts = {label: 0 for label in unique_labels}\n",
    "    unique_labels_mesh_dict = {label: [] for label in unique_labels}\n",
    "\n",
    "    for label, objs in parts:\n",
    "        label_number = label_to_index.get(label)\n",
    "        if label_number is None:\n",
    "            print(f\"Label {label} not found in label_to_index\")\n",
    "            continue\n",
    "        \n",
    "        for obj in objs:\n",
    "            mesh_path = os.path.join(obj_base_dir, f\"{obj}.obj\")\n",
    "            if os.path.exists(mesh_path):\n",
    "                mesh = trimesh.load(mesh_path)\n",
    "                unique_node_name = f\"{label}_{label_counts[label]}\"\n",
    "                unique_node_name = unique_node_name.split('/')[0] + ':' + unique_node_name.split('/')[-1]\n",
    "                obbs_dict[unique_node_name] = mesh.bounds\n",
    "                count_dict[unique_node_name] = mesh.vertices.shape[0]\n",
    "                unique_labels_mesh_dict[label].append(mesh)\n",
    "                label_counts[label] += 1\n",
    "\n",
    "    for label, meshes in unique_labels_mesh_dict.items():\n",
    "        label_number = label_to_index.get(label)\n",
    "        if label_number is None:\n",
    "            print(f\"Label {label} not found in label_to_index\")\n",
    "            continue\n",
    "            \n",
    "        combined_mesh = trimesh.util.concatenate(meshes)\n",
    "        if len(combined_mesh.vertices) > 10000:\n",
    "            save_as_uv_mesh(downsample_mesh(combined_mesh, 10000), 'temp.obj')\n",
    "        else:\n",
    "            save_as_uv_mesh(combined_mesh, 'temp.obj')\n",
    "        \n",
    "        combined_mesh_uv = trimesh.load('temp.obj')\n",
    "        if len(combined_mesh.vertices) > 1024:\n",
    "            trimesh.grouping.merge_vertices(combined_mesh_uv, merge_tex=True)\n",
    "\n",
    "        vertices = combined_mesh_uv.vertices\n",
    "        num_vertices = vertices.shape[0]\n",
    "        labels = np.full((num_vertices, 1), label_number)\n",
    "\n",
    "        combined_vertices.append(vertices)\n",
    "        combined_labels.append(labels)\n",
    "        combined_mesh_list.append(combined_mesh_uv)\n",
    "        \n",
    "    if combined_vertices:\n",
    "        combined_vertices = np.vstack(combined_vertices)\n",
    "        combined_labels = np.vstack(combined_labels)\n",
    "        point_cloud = np.hstack((combined_vertices, combined_labels))\n",
    "    else:\n",
    "        point_cloud = np.array([])\n",
    "\n",
    "    final_mesh = trimesh.util.concatenate(combined_mesh_list)\n",
    "    print(label,len(final_mesh.vertices), point_cloud.shape)\n",
    "    \n",
    "    return point_cloud, final_mesh, obbs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import torch\n",
    "# Assuming the following functions are defined:\n",
    "# load_json, extract_parts, load_parts2obj_uv\n",
    "\n",
    "obj_ids = [\"46531\", \"12159\", \"2554\", \"2355\", \"18233\", \"18277\", \"22981\", \"45187\",\"9067\",\"3512\",\"3810\",\"15719\",\"10207\",\"6490\",\"8795\",'3444','4739']\n",
    "target_dir = '/home/lidosan/Datasets/PartNet_complete'\n",
    "\n",
    "label_to_index = load_json('label2idx.json')\n",
    "\n",
    "\n",
    "def clean_label(label):\n",
    "    \"\"\"Remove specific substrings from the label.\"\"\"\n",
    "    cleaned_label = re.sub(r'\\(other\\)/other_leaf|\\(other\\)|leaf', '', label)\n",
    "    return cleaned_label.strip()\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Convert the label to the desired format and clean it.\"\"\"\n",
    "    replacements = {\n",
    "        \" \": \"_\",\n",
    "        \"/other_\": \"\",\n",
    "        \"sitting_furniture\": \"chair\",\n",
    "        \"pot_body\": \"body\",\n",
    "        \"-\": \"_\",\n",
    "        \"cylinder\": \"decoration\",\n",
    "        # \"/decoration\": \"\",\n",
    "        \"star_shape\": \"star\",\n",
    "        \"sofa_style_arm\": \"arm_sofa_style\",\n",
    "        \"stuffs_contained_in_the_pot\":\"containing_things\",\n",
    "        \"display/screen\":\"display/display_screen\",\n",
    "        \"mug/stuffs_contained\":\"mug/containing_things\"\n",
    "    }\n",
    "    \n",
    "    cleaned_label = clean_label(label).lower()\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        cleaned_label = cleaned_label.replace(old, new)\n",
    "\n",
    "    if \"chair/chair_base/\" in cleaned_label:\n",
    "        cleaned_label = \"chair/chair_base\"\n",
    "    elif \"storage_furniture/cabinet/cabinet_frame\" in cleaned_label and \"bar\" in cleaned_label:\n",
    "        # merge the storage_furniture_bar\n",
    "        cleaned_label = \"storage_furniture/cabinet/cabinet_frame\"\n",
    "    return cleaned_label\n",
    "\n",
    "\n",
    "\n",
    "def extract_parts_v2(data, parent_label=''):\n",
    "    parts = []\n",
    "    if isinstance(data, dict):\n",
    "        if 'text' in data:\n",
    "            label = parent_label + '/' + data['text'] if parent_label else data['text']\n",
    "            label = normalize_label(label)\n",
    "            if 'objs' in data:\n",
    "                parts.append((label, data['objs']))\n",
    "            if 'children' in data:\n",
    "                for child in data['children']:\n",
    "                    parts.extend(extract_parts_v2(child, label))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            parts.extend(extract_parts_v2(item, parent_label))\n",
    "    return parts\n",
    "\n",
    "\n",
    "\n",
    "for obj_id in obj_ids:\n",
    "    number_to_label = {}\n",
    "    obj_base_dir = f'/home/lidosan/Datasets/datasets--ShapeNet--PartNet-archive/blobs/data_v0/{obj_id}'\n",
    "    \n",
    "    obj_structure = load_json(os.path.join(obj_base_dir, 'result.json'))\n",
    "    model_cat = load_json(os.path.join(obj_base_dir, 'meta.json'))['model_cat']\n",
    "    parts_render_img = os.path.join(obj_base_dir, 'parts_render', '0.png')\n",
    "    # Extract parts with labels and object files\n",
    "    parts = extract_parts_v2(obj_structure)\n",
    "    # # Load parts and create the mesh\n",
    "    points, combined_mesh,obbs = load_parts2obj_uv(obj_id, parts, label_to_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    obj_path = os.path.join(target_dir, obj_id)\n",
    "    # if os.path.exists(obj_path):\n",
    "    #     continue\n",
    "    os.makedirs(obj_path, exist_ok=True)\n",
    "    texture_dir = os.path.join(obj_path,'Texture')\n",
    "    # Export mesh\n",
    "    combined_mesh.export(os.path.join(obj_path, f'{obj_id}.obj'))\n",
    "    \n",
    "    #Todo\n",
    "    \n",
    "    # os.makedirs(texture_dir)\n",
    "    # Remove unnecessary files\n",
    "    mtl_file = os.path.join(obj_path, 'material.mtl')\n",
    "    png_file = os.path.join(obj_path, 'material_0.png')\n",
    "    output_json_path = os.path.join(obj_path,'meta.json')\n",
    "    if os.path.exists(mtl_file):\n",
    "        os.remove(mtl_file)\n",
    "    if os.path.exists(png_file):\n",
    "        os.remove(png_file)\n",
    "    \n",
    "    # Export segmentation labels as txt\n",
    "    seg_label = points[:, 3]  # Assuming `pcd` has the point cloud data\n",
    "    np.savetxt(os.path.join(obj_path, f'seg_label.txt'), seg_label, fmt='%d')\n",
    "    np.save(os.path.join(obj_path, f'points.npy'),points[:,:3])\n",
    "    # # Export label to number mapping as json\n",
    "    # with open(os.path.join(obj_path, f'label_to_number.json'), 'w') as f:\n",
    "    #     json.dump(number_to_label, f, indent=4)\n",
    "    \n",
    "    # Copy the render image to the new directory\n",
    "    new_image_path = os.path.join(obj_path, 'parts_render_0.png')\n",
    "    shutil.copy(parts_render_img, new_image_path)\n",
    "    \n",
    "    \n",
    "    # Save label_to_number dictionary as .pth file using torch.save\n",
    "    torch.save(obbs, os.path.join(obj_path, 'obbs.pth'))\n",
    "    \n",
    "    # save npy\n",
    "\n",
    "    # Save the meta result json:\n",
    "    # os.makedirs(obj_path, exist_ok=True)\n",
    "\n",
    "    #TODO use Meshy api to gain the image\n",
    "    with open(output_json_path, 'w') as outfile:\n",
    "        json.dump({'model_cat':model_cat}, outfile, indent=4)\n",
    "\n",
    "    print(model_cat)\n",
    "    \n",
    "print(\"Export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'storage_furniture/cabinet/object/book'.replace('storage_furniture/cabinet/object', 'decoration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1 down sample each part and concat together (accurate everything, but large size still)\n",
    "\n",
    "\n",
    "# case 2 down sample the mesh, and approxmatly estimate the uv and label(doing)\n",
    "\n",
    "\n",
    "number_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seg(filename):\n",
    "    \"\"\"Read and return the contents of the specified file as a list of integers.\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        return [int(line.strip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id='46531'\n",
    "mesh = trimesh.load(f'/home/lidosan/Datasets/PartNet_complete/{id}/{id}.obj')\n",
    "label = np.asarray(load_seg(f'/home/lidosan/Datasets/PartNet_complete/{id}/seg_label.txt'))\n",
    "poitns = np.asarray(mesh.vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh = trimesh.load('path/to/your/mesh.obj')\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Generate random labels for each vertex\n",
    "vertex_labels = label\n",
    "# vertex_uvs = np.asarray(mesh.visual.uv)\n",
    "# Function to downsample vertices\n",
    "def downsample_mesh(mesh, target_num_vertices):\n",
    "    # Use Trimesh's built-in simplification (decimation) method\n",
    "    # The function simplifies the mesh to a target number of faces, not vertices\n",
    "    # So we need to estimate the number of faces that would approximately give us the target number of vertices\n",
    "    target_num_faces = int(target_num_vertices * (len(mesh.faces) / len(mesh.vertices)))\n",
    "    \n",
    "    # Simplify the mesh\n",
    "    simplified_mesh = mesh.simplify_quadratic_decimation(target_num_faces)\n",
    "    \n",
    "    # Return the simplified mesh\n",
    "    return simplified_mesh\n",
    "\n",
    "# Target number of vertices\n",
    "target_num_vertices = 8000  # Set your desired number of vertices\n",
    "\n",
    "# Downsample the mesh\n",
    "simplified_mesh = downsample_mesh(mesh, target_num_vertices)\n",
    "\n",
    "# Create a KDTree for the original and simplified vertices\n",
    "original_vertices = mesh.vertices\n",
    "simplified_vertices = simplified_mesh.vertices\n",
    "\n",
    "tree = cKDTree(original_vertices)\n",
    "\n",
    "# Find the nearest original vertex for each simplified vertex\n",
    "_, indices = tree.query(simplified_vertices)\n",
    "\n",
    "# Map the labels to the simplified vertices\n",
    "simplified_vertex_labels = vertex_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates and labels\n",
    "verts = simplified_vertices\n",
    "labels = simplified_vertex_labels\n",
    "# Normalize labels for coloring\n",
    "max_label = labels.max()\n",
    "colors = plt.get_cmap(\"viridis\")(labels / max_label)[:, :3]\n",
    "\n",
    "# # Create Open3D point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(verts)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([pcd], window_name=\"Point Cloud with Segmentation Labels\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = o3d.io.read_triangle_mesh('/home/lidosan/Workplace-why/OpenIns3D/demo/demo_scene/scannet/scannet_scene2.ply.ply')\n",
    "o3d.visualization.draw_geometries([mesh], window_name=\"Point Cloud with Segmentation Labels\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimesh.load('colored_scene/combined_mesh.ply').show(viewer='gl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Secne test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trans import  * \n",
    "# Define the InstanceObject class\n",
    "class InstanceObject_v2:\n",
    "    def __init__(self, obj_id, category, instance_mesh, scale_factor, obbs, orientation=np.array([0, 1, 0])):\n",
    "        self.obj_id = obj_id\n",
    "        self.category = category\n",
    "        self.mesh = basic_transformation(instance_mesh, scale_factor)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.orientation = orientation\n",
    "\n",
    "        # Initialize position and transformation attributes\n",
    "        self.position = np.zeros(3)\n",
    "        self.rotation_matrix = np.eye(4)\n",
    "        self.translation_matrix = np.eye(4)\n",
    "        # Extract mesh details\n",
    "        # self.contains = list(self.mesh.geometry.keys())\n",
    "        self.obbs = obbs\n",
    "        self.central_points = {}\n",
    "        self.bounds = self.mesh.bounds\n",
    "        self.height = self.bounds[1][2] - self.bounds[0][2]\n",
    "        self.part_list = []  # store all the parts information\n",
    "\n",
    "        # Additional attributes\n",
    "        self.rotation = 0\n",
    "        self.transformation = None\n",
    "        self.objects_on_surface = {}  # Objects on the surfaces\n",
    "\n",
    "\n",
    "    def apply_transformation(self):\n",
    "        final_translation_matrix = create_translation_matrix(self.position)\n",
    "        center_translation_to_origin = create_translation_matrix(-self.mesh.centroid)\n",
    "        center_translation_back = create_translation_matrix(self.mesh.centroid)\n",
    "        final_transform = np.dot(\n",
    "            center_translation_back,\n",
    "            np.dot(create_rotation_matrix('z', self.rotation), center_translation_to_origin)\n",
    "        )\n",
    "        final_transform = np.dot(final_translation_matrix, final_transform)\n",
    "        self.mesh.apply_transform(final_transform)\n",
    "        self.transformation = final_transform\n",
    "        return final_transform\n",
    "\n",
    "    def get_surface_mesh(self):\n",
    "        surface_nodename_map = {\n",
    "            'Bed': ' Mattress_0',\n",
    "            'Table': ' Board_0'\n",
    "        }\n",
    "        surface_nodename = surface_nodename_map[self.category]\n",
    "        transform = self.mesh.graph[surface_nodename][0]\n",
    "        top_mesh = copy.deepcopy(self.mesh.geometry[surface_nodename])\n",
    "        top_mesh.apply_transform(transform)\n",
    "        return top_mesh\n",
    "\n",
    "    def overlap_detection(self, other_mesh):\n",
    "        other_bbox = other_mesh.bounds\n",
    "        for node_name in self.mesh.graph.nodes_geometry:\n",
    "            geometry = self.mesh.geometry[node_name]\n",
    "            transform = self.mesh.graph[node_name][0]\n",
    "            geometry_copy = copy.deepcopy(geometry)\n",
    "            geometry_copy.apply_transform(transform)\n",
    "            self_bbox = geometry_copy.bounds\n",
    "            intersects = not (\n",
    "                self_bbox[1][0] < other_bbox[0][0] or self_bbox[0][0] > other_bbox[1][0] or  # x-axis\n",
    "                self_bbox[1][1] < other_bbox[0][1] or self_bbox[0][1] > other_bbox[1][1] or  # y-axis\n",
    "                self_bbox[1][2] < other_bbox[0][2] or self_bbox[0][2] > other_bbox[1][2]    # z-axis\n",
    "            )\n",
    "            if intersects:\n",
    "                print('Overlap')\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import trimesh\n",
    "from utils.utils import *\n",
    "from utils.trans  import InstanceObject, place_object_on, basic_transformation\n",
    "import shutil\n",
    "\n",
    "# surface_top_list = ['Mug','Hat','Bowl||','Lamp','Vase','Laptop','Keyboard','Bag','Scissors','Display','Knife','Bottle','Earphone']\n",
    "\n",
    "# obejct_with_top_surface_list = ['StorageFurniture','Refrigerator', 'Table','Bed', 'Dishwasher']\n",
    "\n",
    "# obejct_without_top_surface_list =['TrashCan','Chair','Door']\n",
    "\n",
    "\n",
    "\n",
    "surface_top_list = ['Mug','Lamp','Vase','Laptop','Keyboard','Bag','Display','Bottle']\n",
    "# No Knife,Hat,'Display','Scissors','Earphone','Bowl' for now \n",
    "\n",
    "obejct_with_top_surface_list = ['StorageFurniture','Refrigerator', 'Table','Bed', 'Dishwasher']\n",
    "\n",
    "obejct_without_top_surface_list =['TrashCan','Chair','Door']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_factors = {\n",
    "    'Mug': (0.2, 0.3),        # Mugs typically vary a bit in size but are generally small.\n",
    "    'Hat': (0.2, 0.3),        # Hats have some variation but are generally smaller.\n",
    "    'Bowl': (0.2, 0.3),       # Bowls can range from small to medium sizes.\n",
    "    'Lamp': (0.4, 0.5),       # Lamps can range from small desk lamps to larger ones.\n",
    "    'Vase': (0.4, 0.5),       # Vases can vary significantly in size but are usually not as large as laptops.\n",
    "    'Laptop': (0.8, 1.0),     # Laptops have a narrow size range and are relatively larger.\n",
    "    'Keyboard': (0.8, 1.0),   # Keyboards also have a narrow size range similar to laptops.\n",
    "    'Bag': (0.5, 1.0),        # Bags can vary significantly in size.\n",
    "    'Scissors': (0.2, 0.5),   # Scissors typically have a limited size range and are generally small.\n",
    "    'Display': (0.7, 1.0),    # Displays usually have a narrow size range and are relatively larger.\n",
    "    'Knife': (0.2, 0.5),      # Knives can vary from small to medium sizes.\n",
    "    'Bottle': (0.2, 0.5),     # Bottles can vary significantly in size.\n",
    "    'Earphone': (0.2, 0.5)    # Earphones are generally very small.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_json('scene_layout.json')\n",
    "all_obj_dict = load_json('obj_correspondence_tiny.json')\n",
    "large_id_list = [obj['obj_id'] for obj in config['large_objects']]\n",
    "large_objects_contain = {}\n",
    "tiny_obj_id_list = []\n",
    "obj_num = 1\n",
    "for obj_id in [obj['obj_id'] for obj in config['large_objects']]:\n",
    "    tiny_objects = []\n",
    "    for i in range(obj_num):\n",
    "        # still random\n",
    "        tiny_obj_cat, tiny_obj_id= random_objects_generate(surface_top_list,all_obj_dict)\n",
    "        tiny_obj_info = {\n",
    "            \"obj_id\":tiny_obj_id,\n",
    "            \"category\":tiny_obj_cat,\n",
    "            \"scale_factor\":random.uniform(cat_factors[tiny_obj_cat][0],cat_factors[tiny_obj_cat][1])\n",
    "        }\n",
    "        tiny_objects.append(tiny_obj_info)\n",
    "        tiny_obj_id_list.append(tiny_obj_id)\n",
    "    large_objects_contain[obj_id] = tiny_objects\n",
    "id_list = large_id_list+tiny_obj_id_list\n",
    "\n",
    "# 读取所有的点云\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene_object_v2(id_list): \n",
    "    point_clouds = {}\n",
    "    mesh_dict = {}\n",
    "    label_to_number = {}\n",
    "    cat_to_id = {}\n",
    "    for obj_id in id_list:\n",
    "        # print(obj_id)\n",
    "        obj_base_dir = '/home/lidosan/Datasets/PartNet_complete/{}'.format(obj_id)\n",
    "        obj_structure = load_json(os.path.join(obj_base_dir, 'label_to_number.json'))\n",
    "        # obj_infromation = load_json(os.path.join(obj_base_dir, 'meta.json'))\n",
    "        # Extract parts with labels and object files\n",
    "        parts = extract_parts(obj_structure)\n",
    "\n",
    "        # Load first object\n",
    "        point_cloud, label_to_number, combined_mesh = load_and_color_meshes_semantics(obj_id, parts, label_to_number)\n",
    "\n",
    "        mesh_dict[obj_id] = combined_mesh\n",
    "        point_clouds[obj_id] = point_cloud\n",
    "        cat_to_id[obj_id] = obj_infromation['model_cat']\n",
    "    return point_clouds, mesh_dict, label_to_number,cat_to_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_json(os.path.join(obj_base_dir, 'meta.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "print(\"Make sure your file's visibility \"+'in Google Drive is set to \"Anyone with the link\"')\n",
    "input_link = 'https://drive.google.com/file/d/1Y3R1nu1uO66VD4RmXHGhiKMqztCnEkOK/view?usp=drive_link'\n",
    "input_link = input_link[32:]\n",
    "end = input_link.index('/')\n",
    "ID = input_link[:end]\n",
    "download_link = 'https://drive.google.com/uc?export=download&id='+ID\n",
    "print('This Your Direct Download Link : ',download_link)\n",
    "# ok = False\n",
    "# while not(ok) :\n",
    "#     prompt = input('You Wanna Download The File Right Now ?  Y/N : ')\n",
    "#     ok = prompt =='Y' or prompt =='N' or prompt =='n' or prompt =='y'\n",
    "\n",
    "prompt = 'N'\n",
    "if prompt == 'Y' or prompt == 'y'  :\n",
    "    webbrowser.open(download_link)\n",
    "    \n",
    "else :\n",
    "    print('thank u for your time <3 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "download_link = 'https://drive.google.com/uc?export=download&id=1Y3R1nu1uO66VD4RmXHGhiKMqztCnEkOK'\n",
    "response = urlopen(download_link)\n",
    "filename = response.headers\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_link = 'https://drive.google.com/uc?export=download&id=1Y3R1nu1uO66VD4RmXHGhiKMqztCnEkOK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open(download_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(download_link,'.obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api = 'msy_nkfrIXgu8woN4Ei9bgs3MJ3QcMISwdvhNsTK'\n",
    "download_link = 'https://2015.filemail.com/api/file/get?filekey=xmHqeumxh8Qe-lICNOFw-G-sLemA5uC0NWZ2Nchf3R7Mq35TCtD-bZ9n&pk_vid=dbb08defd6f73cdf17217318673f7d58'\n",
    "payload = {\n",
    "    \"model_url\": f\"{download_link}\",\n",
    "    \"object_prompt\": \"a monster mask\",\n",
    "    \"style_prompt\": \"red fangs, Samurai outfit that fused with japanese batik style\",\n",
    "    \"enable_original_uv\": True,\n",
    "    \"enable_pbr\": False,\n",
    "    \"resolution\": \"2048\",\n",
    "    \"negative_prompt\": \"low quality, low resolution, low poly, ugly\"\n",
    "}\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api}\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://api.meshy.ai/v1/text-to-texture\",\n",
    "    headers=headers,\n",
    "    json=payload,\n",
    ")\n",
    "response.raise_for_status()\n",
    "print(response.json())\n",
    "\n",
    "while True:\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api}\"\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    f\"https://api.meshy.ai/v1/text-to-texture/{task_id}\",\n",
    "    headers=headers,\n",
    ")\n",
    "response.raise_for_status()\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['texture_urls']['base_color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://assets.meshy.ai/aae3bece-62bf-4850-b451-26f3ed655929/tasks/0190deca-b556-704f-bed1-0964471a348a/output/preview.png?Expires=1721984010&Signature=TDnpe0kcjUEroofzgCwfGeY9kA3s6jaNTJPhN-8gzxe3wsdUF-gIHae~8UZ06KTCjN~nWKLvoZfLm1559TC0y9lxQ7jLEdsueqvcHavOpSg0152XqCAvYwIS8DCzETgps1MToS4uPrYqTyONynwJmyD5MeYnj995yAOhJ2bvctfM4dAgHewDsxNvyQ9Jp1-gdds-IgrtLC7wIeKaaDofeMGYRY4X7MDNawq8da5SDCldbDrN-PIX0C2QT62ROdr5HKl9hLugqTZdO5RUlGLzCmXbd52PjYDkJxBRIjy3sokpvhqDaKNyFqre5TvQXlK7o4KaN3S39Rrb4F-hvHo7cw__&Key-Pair-Id=KL5I0C8H7HX83\", \"thumbnail.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use meshy api to generate the texture\n",
    "\n",
    "# header请求头\n",
    "headers = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36',\n",
    "  'Cookie': cookies # 填入网盘账号的cookie\n",
    "}\n",
    "\n",
    "# 调用网盘api(method=download即下载，还有很多其他方法，请参考百度开发者api)\n",
    "url = 'https://pcs.baidu.com/rest/2.0/pcs/file?method=download&app_id=309847&path=/[网盘文件路径]/xxx文件'\n",
    "\n",
    "# 发送get请求\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 响应状态码\n",
    "print(response.status_code)\n",
    "\n",
    "# 响应内容\n",
    "print(response.text)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import  Image\n",
    "img = Image.open('meshy_texture/Image_0.jpg')\n",
    "c_mesh  = color_the_mesh(combined_mesh,img,combined_mesh.visual.uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mesh.visual.kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mesh.visual = c_mesh.visual.to_color()\n",
    "c_mesh.visual.vertex_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts = np.asarray(c_mesh.vertices)\n",
    "color = np.asarray(c_mesh.visual.vertex_colors)\n",
    "colors = color[:, :3] / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Extract coordinates and labels\n",
    "# points = pcd[:,:3]# # Extract coordinates and labels\n",
    "# points = pcd[:,:3]\n",
    "# labels = pcd[:,3]\n",
    "# # Normalize labels for coloring\n",
    "# max_label = labels.max()\n",
    "# colors = plt.get_cmap(\"viridis\")(labels / max_label)[:, :3]\n",
    "\n",
    "# Create Open3D point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(verts)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([pcd], window_name=\"Point Cloud with Segmentation Labels\", width=800, height=600)\n",
    "# colors = plt.get_cmap(\"viridis\")(labels / max_label)[:, :3]\n",
    "\n",
    "# Create Open3D point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(verts)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([pcd], window_name=\"Point Cloud with Segmentation Labels\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(cabniet.faces).astype('uint32') == indices\n",
    "# cabniet faces has been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = point_cloud_[:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_new = cabniet.faces\n",
    "indices_old = indices\n",
    "differing_indices = []\n",
    "old_labels = lables \n",
    "new_labels = copy.deepcopy(labels)\n",
    "\n",
    "for i in range(indices_one.shape[0]):\n",
    "    for j in range(indices_one.shape[1]):\n",
    "        if indices_one[i, j] != indices_two[i, j]:\n",
    "            differing_indices.append((i, j, indices_one[i, j], indices_two[i, j]))\n",
    "            new_labels[indices_one[i, j]] = old_labels[indices_two[i,j]]\n",
    "            \n",
    "\n",
    "# # Display differing indices\n",
    "# for diff in differing_indices:\n",
    "#     print(f\"Difference at row {diff[0]}, column {diff[1]}: indices_one={diff[2]}, indices_two={diff[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_labels),len(old_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create Open3D point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# Visualize the point cloud\n",
    "o3d.visualization.draw_geometries([pcd], window_name=\"Point Cloud with Segmentation Labels\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openins3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
